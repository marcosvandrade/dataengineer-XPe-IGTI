{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7836d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Aula 2.4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4415d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesRdd = spark.sparkContext.textFile(\"README.md\")\n",
    "\n",
    "linesRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0e3957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('# Apache Spark', 14),\n",
       " ('', 0),\n",
       " ('Spark is a unified analytics engine for large-scale data processing. It provides',\n",
       "  80),\n",
       " ('high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       "  75),\n",
       " ('supports general computation graphs for data analysis. It also supports a',\n",
       "  73),\n",
       " ('rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       "  74),\n",
       " ('pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing,',\n",
       "  98),\n",
       " ('and Structured Streaming for stream processing.', 47),\n",
       " ('', 0),\n",
       " ('<https://spark.apache.org/>', 27),\n",
       " ('', 0),\n",
       " ('[![GitHub Actions Build](https://github.com/apache/spark/actions/workflows/build_main.yml/badge.svg)](https://github.com/apache/spark/actions/workflows/build_main.yml)',\n",
       "  167),\n",
       " ('[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       "  189),\n",
       " ('[![PySpark Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)',\n",
       "  123),\n",
       " ('[![PyPI Downloads](https://static.pepy.tech/personalized-badge/pyspark?period=month&units=international_system&left_color=black&right_color=orange&left_text=PyPI%20downloads)](https://pypi.org/project/pyspark/)',\n",
       "  210),\n",
       " ('', 0),\n",
       " ('', 0),\n",
       " ('## Online Documentation', 23),\n",
       " ('', 0),\n",
       " ('You can find the latest Spark documentation, including a programming', 68),\n",
       " ('guide, on the [project web page](https://spark.apache.org/documentation.html).',\n",
       "  78),\n",
       " ('This README file only contains basic setup instructions.', 56),\n",
       " ('', 0),\n",
       " ('## Building Spark', 17),\n",
       " ('', 0),\n",
       " ('Spark is built using [Apache Maven](https://maven.apache.org/).', 63),\n",
       " ('To build Spark and its example programs, run:', 45),\n",
       " ('', 0),\n",
       " ('```bash', 7),\n",
       " ('./build/mvn -DskipTests clean package', 37),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('(You do not need to do this if you downloaded a pre-built package.)', 67),\n",
       " ('', 0),\n",
       " ('More detailed documentation is available from the project site, at', 66),\n",
       " ('[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
       "  77),\n",
       " ('', 0),\n",
       " ('For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).',\n",
       "  157),\n",
       " ('', 0),\n",
       " ('## Interactive Scala Shell', 26),\n",
       " ('', 0),\n",
       " ('The easiest way to start using Spark is through the Scala shell:', 64),\n",
       " ('', 0),\n",
       " ('```bash', 7),\n",
       " ('./bin/spark-shell', 17),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('Try the following command, which should return 1,000,000,000:', 61),\n",
       " ('', 0),\n",
       " ('```scala', 8),\n",
       " ('scala> spark.range(1000 * 1000 * 1000).count()', 46),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('## Interactive Python Shell', 27),\n",
       " ('', 0),\n",
       " ('Alternatively, if you prefer Python, you can use the Python shell:', 66),\n",
       " ('', 0),\n",
       " ('```bash', 7),\n",
       " ('./bin/pyspark', 13),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('And run the following command, which should also return 1,000,000,000:',\n",
       "  70),\n",
       " ('', 0),\n",
       " ('```python', 9),\n",
       " ('>>> spark.range(1000 * 1000 * 1000).count()', 43),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('## Example Programs', 19),\n",
       " ('', 0),\n",
       " ('Spark also comes with several sample programs in the `examples` directory.',\n",
       "  74),\n",
       " ('To run one of them, use `./bin/run-example <class> [params]`. For example:',\n",
       "  74),\n",
       " ('', 0),\n",
       " ('```bash', 7),\n",
       " ('./bin/run-example SparkPi', 25),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('will run the Pi example locally.', 32),\n",
       " ('', 0),\n",
       " ('You can set the MASTER environment variable when running examples to submit',\n",
       "  75),\n",
       " ('examples to a cluster. This can be a mesos:// or spark:// URL,', 62),\n",
       " ('\"yarn\" to run on YARN, and \"local\" to run', 41),\n",
       " ('locally with one thread, or \"local[N]\" to run locally with N threads. You',\n",
       "  73),\n",
       " ('can also use an abbreviated class name if the class is in the `examples`',\n",
       "  72),\n",
       " ('package. For instance:', 22),\n",
       " ('', 0),\n",
       " ('```bash', 7),\n",
       " ('MASTER=spark://host:7077 ./bin/run-example SparkPi', 50),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('Many of the example programs print usage help if no params are given.', 69),\n",
       " ('', 0),\n",
       " ('## Running Tests', 16),\n",
       " ('', 0),\n",
       " ('Testing first requires [building Spark](#building-spark). Once Spark is built, tests',\n",
       "  84),\n",
       " ('can be run using:', 17),\n",
       " ('', 0),\n",
       " ('```bash', 7),\n",
       " ('./dev/run-tests', 15),\n",
       " ('```', 3),\n",
       " ('', 0),\n",
       " ('Please see the guidance on how to', 33),\n",
       " ('[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
       "  110),\n",
       " ('', 0),\n",
       " ('There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md',\n",
       "  105),\n",
       " ('', 0),\n",
       " ('## A Note About Hadoop Versions', 31),\n",
       " ('', 0),\n",
       " ('Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported',\n",
       "  77),\n",
       " ('storage systems. Because the protocols have changed in different versions of',\n",
       "  76),\n",
       " ('Hadoop, you must build Spark against the same version that your cluster runs.',\n",
       "  77),\n",
       " ('', 0),\n",
       " ('Please refer to the build documentation at', 42),\n",
       " ('[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       "  157),\n",
       " ('for detailed guidance on building for a particular distribution of Hadoop, including',\n",
       "  84),\n",
       " ('building for particular Hive and Hive Thriftserver distributions.', 65),\n",
       " ('', 0),\n",
       " ('## Configuration', 16),\n",
       " ('', 0),\n",
       " ('Please refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)',\n",
       "  98),\n",
       " ('in the online documentation for an overview on how to configure Spark.',\n",
       "  70),\n",
       " ('', 0),\n",
       " ('## Contributing', 15),\n",
       " ('', 0),\n",
       " ('Please review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)',\n",
       "  91),\n",
       " ('for information on how to get started contributing to the project.', 66)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map\n",
    "mapRdd = linesRdd.map(lambda line: (line, len(line)))\n",
    "mapRdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19e0b16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'a',\n",
       " 'unified',\n",
       " 'analytics',\n",
       " 'engine',\n",
       " 'for',\n",
       " 'large-scale',\n",
       " 'data',\n",
       " 'processing.',\n",
       " 'It',\n",
       " 'provides',\n",
       " 'high-level',\n",
       " 'APIs',\n",
       " 'in',\n",
       " 'Scala,',\n",
       " 'Java,',\n",
       " 'Python,',\n",
       " 'and',\n",
       " 'R,',\n",
       " 'and',\n",
       " 'an',\n",
       " 'optimized',\n",
       " 'engine',\n",
       " 'that',\n",
       " 'supports',\n",
       " 'general',\n",
       " 'computation',\n",
       " 'graphs',\n",
       " 'for',\n",
       " 'data',\n",
       " 'analysis.',\n",
       " 'It',\n",
       " 'also',\n",
       " 'supports',\n",
       " 'a',\n",
       " 'rich',\n",
       " 'set',\n",
       " 'of',\n",
       " 'higher-level',\n",
       " 'tools',\n",
       " 'including',\n",
       " 'Spark',\n",
       " 'SQL',\n",
       " 'for',\n",
       " 'SQL',\n",
       " 'and',\n",
       " 'DataFrames,',\n",
       " 'pandas',\n",
       " 'API',\n",
       " 'on',\n",
       " 'Spark',\n",
       " 'for',\n",
       " 'pandas',\n",
       " 'workloads,',\n",
       " 'MLlib',\n",
       " 'for',\n",
       " 'machine',\n",
       " 'learning,',\n",
       " 'GraphX',\n",
       " 'for',\n",
       " 'graph',\n",
       " 'processing,',\n",
       " 'and',\n",
       " 'Structured',\n",
       " 'Streaming',\n",
       " 'for',\n",
       " 'stream',\n",
       " 'processing.',\n",
       " '<https://spark.apache.org/>',\n",
       " '[![GitHub',\n",
       " 'Actions',\n",
       " 'Build](https://github.com/apache/spark/actions/workflows/build_main.yml/badge.svg)](https://github.com/apache/spark/actions/workflows/build_main.yml)',\n",
       " '[![AppVeyor',\n",
       " 'Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       " '[![PySpark',\n",
       " 'Coverage](https://codecov.io/gh/apache/spark/branch/master/graph/badge.svg)](https://codecov.io/gh/apache/spark)',\n",
       " '[![PyPI',\n",
       " 'Downloads](https://static.pepy.tech/personalized-badge/pyspark?period=month&units=international_system&left_color=black&right_color=orange&left_text=PyPI%20downloads)](https://pypi.org/project/pyspark/)',\n",
       " '##',\n",
       " 'Online',\n",
       " 'Documentation',\n",
       " 'You',\n",
       " 'can',\n",
       " 'find',\n",
       " 'the',\n",
       " 'latest',\n",
       " 'Spark',\n",
       " 'documentation,',\n",
       " 'including',\n",
       " 'a',\n",
       " 'programming',\n",
       " 'guide,',\n",
       " 'on',\n",
       " 'the',\n",
       " '[project',\n",
       " 'web',\n",
       " 'page](https://spark.apache.org/documentation.html).',\n",
       " 'This',\n",
       " 'README',\n",
       " 'file',\n",
       " 'only',\n",
       " 'contains',\n",
       " 'basic',\n",
       " 'setup',\n",
       " 'instructions.',\n",
       " '##',\n",
       " 'Building',\n",
       " 'Spark',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'built',\n",
       " 'using',\n",
       " '[Apache',\n",
       " 'Maven](https://maven.apache.org/).',\n",
       " 'To',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'and',\n",
       " 'its',\n",
       " 'example',\n",
       " 'programs,',\n",
       " 'run:',\n",
       " '```bash',\n",
       " './build/mvn',\n",
       " '-DskipTests',\n",
       " 'clean',\n",
       " 'package',\n",
       " '```',\n",
       " '(You',\n",
       " 'do',\n",
       " 'not',\n",
       " 'need',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this',\n",
       " 'if',\n",
       " 'you',\n",
       " 'downloaded',\n",
       " 'a',\n",
       " 'pre-built',\n",
       " 'package.)',\n",
       " 'More',\n",
       " 'detailed',\n",
       " 'documentation',\n",
       " 'is',\n",
       " 'available',\n",
       " 'from',\n",
       " 'the',\n",
       " 'project',\n",
       " 'site,',\n",
       " 'at',\n",
       " '[\"Building',\n",
       " 'Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
       " 'For',\n",
       " 'general',\n",
       " 'development',\n",
       " 'tips,',\n",
       " 'including',\n",
       " 'info',\n",
       " 'on',\n",
       " 'developing',\n",
       " 'Spark',\n",
       " 'using',\n",
       " 'an',\n",
       " 'IDE,',\n",
       " 'see',\n",
       " '[\"Useful',\n",
       " 'Developer',\n",
       " 'Tools\"](https://spark.apache.org/developer-tools.html).',\n",
       " '##',\n",
       " 'Interactive',\n",
       " 'Scala',\n",
       " 'Shell',\n",
       " 'The',\n",
       " 'easiest',\n",
       " 'way',\n",
       " 'to',\n",
       " 'start',\n",
       " 'using',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'through',\n",
       " 'the',\n",
       " 'Scala',\n",
       " 'shell:',\n",
       " '```bash',\n",
       " './bin/spark-shell',\n",
       " '```',\n",
       " 'Try',\n",
       " 'the',\n",
       " 'following',\n",
       " 'command,',\n",
       " 'which',\n",
       " 'should',\n",
       " 'return',\n",
       " '1,000,000,000:',\n",
       " '```scala',\n",
       " 'scala>',\n",
       " 'spark.range(1000',\n",
       " '*',\n",
       " '1000',\n",
       " '*',\n",
       " '1000).count()',\n",
       " '```',\n",
       " '##',\n",
       " 'Interactive',\n",
       " 'Python',\n",
       " 'Shell',\n",
       " 'Alternatively,',\n",
       " 'if',\n",
       " 'you',\n",
       " 'prefer',\n",
       " 'Python,',\n",
       " 'you',\n",
       " 'can',\n",
       " 'use',\n",
       " 'the',\n",
       " 'Python',\n",
       " 'shell:',\n",
       " '```bash',\n",
       " './bin/pyspark',\n",
       " '```',\n",
       " 'And',\n",
       " 'run',\n",
       " 'the',\n",
       " 'following',\n",
       " 'command,',\n",
       " 'which',\n",
       " 'should',\n",
       " 'also',\n",
       " 'return',\n",
       " '1,000,000,000:',\n",
       " '```python',\n",
       " '>>>',\n",
       " 'spark.range(1000',\n",
       " '*',\n",
       " '1000',\n",
       " '*',\n",
       " '1000).count()',\n",
       " '```',\n",
       " '##',\n",
       " 'Example',\n",
       " 'Programs',\n",
       " 'Spark',\n",
       " 'also',\n",
       " 'comes',\n",
       " 'with',\n",
       " 'several',\n",
       " 'sample',\n",
       " 'programs',\n",
       " 'in',\n",
       " 'the',\n",
       " '`examples`',\n",
       " 'directory.',\n",
       " 'To',\n",
       " 'run',\n",
       " 'one',\n",
       " 'of',\n",
       " 'them,',\n",
       " 'use',\n",
       " '`./bin/run-example',\n",
       " '<class>',\n",
       " '[params]`.',\n",
       " 'For',\n",
       " 'example:',\n",
       " '```bash',\n",
       " './bin/run-example',\n",
       " 'SparkPi',\n",
       " '```',\n",
       " 'will',\n",
       " 'run',\n",
       " 'the',\n",
       " 'Pi',\n",
       " 'example',\n",
       " 'locally.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'set',\n",
       " 'the',\n",
       " 'MASTER',\n",
       " 'environment',\n",
       " 'variable',\n",
       " 'when',\n",
       " 'running',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'submit',\n",
       " 'examples',\n",
       " 'to',\n",
       " 'a',\n",
       " 'cluster.',\n",
       " 'This',\n",
       " 'can',\n",
       " 'be',\n",
       " 'a',\n",
       " 'mesos://',\n",
       " 'or',\n",
       " 'spark://',\n",
       " 'URL,',\n",
       " '\"yarn\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'on',\n",
       " 'YARN,',\n",
       " 'and',\n",
       " '\"local\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'locally',\n",
       " 'with',\n",
       " 'one',\n",
       " 'thread,',\n",
       " 'or',\n",
       " '\"local[N]\"',\n",
       " 'to',\n",
       " 'run',\n",
       " 'locally',\n",
       " 'with',\n",
       " 'N',\n",
       " 'threads.',\n",
       " 'You',\n",
       " 'can',\n",
       " 'also',\n",
       " 'use',\n",
       " 'an',\n",
       " 'abbreviated',\n",
       " 'class',\n",
       " 'name',\n",
       " 'if',\n",
       " 'the',\n",
       " 'class',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " '`examples`',\n",
       " 'package.',\n",
       " 'For',\n",
       " 'instance:',\n",
       " '```bash',\n",
       " 'MASTER=spark://host:7077',\n",
       " './bin/run-example',\n",
       " 'SparkPi',\n",
       " '```',\n",
       " 'Many',\n",
       " 'of',\n",
       " 'the',\n",
       " 'example',\n",
       " 'programs',\n",
       " 'print',\n",
       " 'usage',\n",
       " 'help',\n",
       " 'if',\n",
       " 'no',\n",
       " 'params',\n",
       " 'are',\n",
       " 'given.',\n",
       " '##',\n",
       " 'Running',\n",
       " 'Tests',\n",
       " 'Testing',\n",
       " 'first',\n",
       " 'requires',\n",
       " '[building',\n",
       " 'Spark](#building-spark).',\n",
       " 'Once',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'built,',\n",
       " 'tests',\n",
       " 'can',\n",
       " 'be',\n",
       " 'run',\n",
       " 'using:',\n",
       " '```bash',\n",
       " './dev/run-tests',\n",
       " '```',\n",
       " 'Please',\n",
       " 'see',\n",
       " 'the',\n",
       " 'guidance',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " '[run',\n",
       " 'tests',\n",
       " 'for',\n",
       " 'a',\n",
       " 'module,',\n",
       " 'or',\n",
       " 'individual',\n",
       " 'tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
       " 'There',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'Kubernetes',\n",
       " 'integration',\n",
       " 'test,',\n",
       " 'see',\n",
       " 'resource-managers/kubernetes/integration-tests/README.md',\n",
       " '##',\n",
       " 'A',\n",
       " 'Note',\n",
       " 'About',\n",
       " 'Hadoop',\n",
       " 'Versions',\n",
       " 'Spark',\n",
       " 'uses',\n",
       " 'the',\n",
       " 'Hadoop',\n",
       " 'core',\n",
       " 'library',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'to',\n",
       " 'HDFS',\n",
       " 'and',\n",
       " 'other',\n",
       " 'Hadoop-supported',\n",
       " 'storage',\n",
       " 'systems.',\n",
       " 'Because',\n",
       " 'the',\n",
       " 'protocols',\n",
       " 'have',\n",
       " 'changed',\n",
       " 'in',\n",
       " 'different',\n",
       " 'versions',\n",
       " 'of',\n",
       " 'Hadoop,',\n",
       " 'you',\n",
       " 'must',\n",
       " 'build',\n",
       " 'Spark',\n",
       " 'against',\n",
       " 'the',\n",
       " 'same',\n",
       " 'version',\n",
       " 'that',\n",
       " 'your',\n",
       " 'cluster',\n",
       " 'runs.',\n",
       " 'Please',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " 'build',\n",
       " 'documentation',\n",
       " 'at',\n",
       " '[\"Specifying',\n",
       " 'the',\n",
       " 'Hadoop',\n",
       " 'Version',\n",
       " 'and',\n",
       " 'Enabling',\n",
       " 'YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       " 'for',\n",
       " 'detailed',\n",
       " 'guidance',\n",
       " 'on',\n",
       " 'building',\n",
       " 'for',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'distribution',\n",
       " 'of',\n",
       " 'Hadoop,',\n",
       " 'including',\n",
       " 'building',\n",
       " 'for',\n",
       " 'particular',\n",
       " 'Hive',\n",
       " 'and',\n",
       " 'Hive',\n",
       " 'Thriftserver',\n",
       " 'distributions.',\n",
       " '##',\n",
       " 'Configuration',\n",
       " 'Please',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'the',\n",
       " '[Configuration',\n",
       " 'Guide](https://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'in',\n",
       " 'the',\n",
       " 'online',\n",
       " 'documentation',\n",
       " 'for',\n",
       " 'an',\n",
       " 'overview',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'configure',\n",
       " 'Spark.',\n",
       " '##',\n",
       " 'Contributing',\n",
       " 'Please',\n",
       " 'review',\n",
       " 'the',\n",
       " '[Contribution',\n",
       " 'to',\n",
       " 'Spark',\n",
       " 'guide](https://spark.apache.org/contributing.html)',\n",
       " 'for',\n",
       " 'information',\n",
       " 'on',\n",
       " 'how',\n",
       " 'to',\n",
       " 'get',\n",
       " 'started',\n",
       " 'contributing',\n",
       " 'to',\n",
       " 'the',\n",
       " 'project.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatMap\n",
    "mapRdd = linesRdd.flatMap(lambda line: line.split())\n",
    "mapRdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6efd2050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'analytics',\n",
       " 'and',\n",
       " 'and',\n",
       " 'an',\n",
       " 'analysis.',\n",
       " 'also',\n",
       " 'a',\n",
       " 'and',\n",
       " 'and',\n",
       " 'a',\n",
       " 'and',\n",
       " 'a',\n",
       " 'available',\n",
       " 'at',\n",
       " 'an',\n",
       " 'also',\n",
       " 'also',\n",
       " 'a',\n",
       " 'a',\n",
       " 'and',\n",
       " 'also',\n",
       " 'an',\n",
       " 'abbreviated',\n",
       " 'are',\n",
       " 'a',\n",
       " 'also',\n",
       " 'a',\n",
       " 'and',\n",
       " 'against',\n",
       " 'at',\n",
       " 'and',\n",
       " 'a',\n",
       " 'and',\n",
       " 'an']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter\n",
    "# flatMap\n",
    "filterRdd = linesRdd.flatMap(lambda line: line.split()) \\\n",
    "                    .filter(lambda word : word.startswith(\"a\"))\n",
    "filterRdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8361050b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tres', 1), ('um', 2), ('dois', 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = [\"um\", \"um\", \"dois\", \"tres\"]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(lista)\n",
    "\n",
    "rdd2 = rdd.map(lambda w: (w, 1)) \\\n",
    "          .reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "rdd2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "913ba5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dois', 1), ('tres', 1), ('um', 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey\n",
    "lista = [\"um\", \"um\", \"dois\", \"tres\"]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(lista)\n",
    "\n",
    "rdd2 = rdd.map(lambda w: (w, 1)) \\\n",
    "          .reduceByKey(lambda a,b: a+b) \\\n",
    "          .sortByKey(\"asc\")\n",
    "\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33aefb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tres', 1), ('dois', 1), ('um', 2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortBy\n",
    "lista = [\"um\", \"um\", \"dois\", \"tres\"]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(lista)\n",
    "\n",
    "rdd2 = rdd.map(lambda w: (w, 1)) \\\n",
    "          .reduceByKey(lambda a,b: a+b) \\\n",
    "          .sortBy(lambda t: t[1])\n",
    "\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "392033d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['um', 'um', 'dois', 'tres', 'quatro', 'cinco']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union\n",
    "lista1 = [\"um\", \"um\", \"dois\", \"tres\"]\n",
    "lista2 = [\"quatro\", \"cinco\"]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(lista1)\n",
    "rdd2 = spark.sparkContext.parallelize(lista2)\n",
    "\n",
    "rddUnion = rdd1.union(rdd2)\n",
    "rddUnion.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0d5898d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['um']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intersection\n",
    "\n",
    "lista1 = [\"um\", \"um\", \"dois\", \"tres\"]\n",
    "lista2 = [\"um\", \"quatro\", \"cinco\"]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(lista1)\n",
    "rdd2 = spark.sparkContext.parallelize(lista2)\n",
    "\n",
    "rddUnion = rdd1.intersection(rdd2)\n",
    "rddUnion.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2bc836d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tres', 'um', 'dois']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct\n",
    "lista1 = [\"um\", \"um\", \"dois\", \"tres\"]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(lista1)\n",
    "rddDistinct = rdd1.distinct()\n",
    "\n",
    "rddDistinct.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61649050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Maria', (30, 'SP')), ('Pedro', (39, 'BH'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join\n",
    "lista1 = [(\"Pedro\", 39), (\"Maria\", 30)]\n",
    "lista2 = [(\"Pedro\", \"BH\"), (\"Maria\", \"SP\"), (\"João\", \"RJ\")]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(lista1)\n",
    "rdd2 = spark.sparkContext.parallelize(lista2)\n",
    "\n",
    "rddJoin = rdd1.join(rdd2)\n",
    "\n",
    "rddJoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1200b863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[74] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos falar de ações?\n",
    "\n",
    "# join\n",
    "lista1 = [(\"Pedro\", 39), (\"Maria\", 30)]\n",
    "lista2 = [(\"Pedro\", \"BH\"), (\"Maria\", \"SP\"), (\"João\", \"RJ\")]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(lista1)\n",
    "rdd2 = spark.sparkContext.parallelize(lista2)\n",
    "\n",
    "rddJoin = rdd1.join(rdd2)\n",
    "\n",
    "rddJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7f7f6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Maria', (30, 'SP')), ('Pedro', (39, 'BH'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddJoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a14431be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count \n",
    "rddJoin.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62db6e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Maria', (30, 'SP'))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take\n",
    "rddJoin.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81aad12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['um', 'um', 'um']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top\n",
    "\n",
    "lista1 = [\"um\", \"um\", \"dois\", \"tres\"]\n",
    "lista2 = [\"um\", \"quatro\", \"cinco\"]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(lista1)\n",
    "rdd2 = spark.sparkContext.parallelize(lista2)\n",
    "\n",
    "rddUnion = rdd1.union(rdd2)\n",
    "rddUnion.collect()\n",
    "\n",
    "rddUnion.top(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05f3ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'um': 3, 'dois': 1, 'tres': 1, 'quatro': 1, 'cinco': 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countByValue\n",
    "\n",
    "lista1 = [\"um\", \"um\", \"dois\", \"tres\"]\n",
    "lista2 = [\"um\", \"quatro\", \"cinco\"]\n",
    "\n",
    "rdd1 = spark.sparkContext.parallelize(lista1)\n",
    "rdd2 = spark.sparkContext.parallelize(lista2)\n",
    "\n",
    "rddUnion = rdd1.union(rdd2)\n",
    "rddUnion.collect()\n",
    "\n",
    "rddUnion.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb07c97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rddUnion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PEDRO~1.GUE\\AppData\\Local\\Temp/ipykernel_9420/4194739258.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrddUnion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'saida3.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'rddUnion' is not defined"
     ]
    }
   ],
   "source": [
    "rddUnion.saveAsTextFile('saida3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699aab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
