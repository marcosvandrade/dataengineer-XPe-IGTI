{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark entry point\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Aula Interativa 2 - Apache Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Arquivo https://www.kaggle.com/c/titanic/data?select=train.csv\n",
    "titanic_df = spark.read.csv('/home/pcalais/XPE/engenharia-dados/aula2/titanic-3.csv',header='True',inferSchema='True')\n",
    "\n",
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.groupBy('survived').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|survived|count(1)|\n",
      "+--------+--------+\n",
      "|       1|     342|\n",
      "|       0|     549|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.createOrReplaceTempView('table')\n",
    "\n",
    "spark.sql(\"SELECT survived, count(*) FROM table GROUP BY survived\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|survived|         avg(Fare)|\n",
      "+--------+------------------+\n",
      "|       1| 48.39540760233917|\n",
      "|       0|22.117886885245877|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.groupBy('survived').agg({\"Fare\": \"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|survived|         avg(fare)|\n",
      "+--------+------------------+\n",
      "|       1| 48.39540760233917|\n",
      "|       0|22.117886885245877|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT survived, avg(fare) FROM table GROUP BY survived\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "| age|age_count|\n",
      "+----+---------+\n",
      "|null|       52|\n",
      "|24.0|       15|\n",
      "|35.0|       11|\n",
      "|27.0|       11|\n",
      "|36.0|       11|\n",
      "|22.0|       11|\n",
      "|30.0|       10|\n",
      "|18.0|        9|\n",
      "|32.0|        9|\n",
      "|19.0|        9|\n",
      "|31.0|        8|\n",
      "|29.0|        8|\n",
      "| 4.0|        7|\n",
      "|28.0|        7|\n",
      "|34.0|        6|\n",
      "|25.0|        6|\n",
      "|42.0|        6|\n",
      "|40.0|        6|\n",
      "|48.0|        6|\n",
      "|33.0|        6|\n",
      "+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT age, count(*) as age_count FROM table WHERE survived == 1 GROUP BY age ORDER BY age_count DESC\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/22 21:36:27 WARN SimpleFunctionRegistry: The function upperudf replaced a previously registered function.\n",
      "+--------------------+--------------------+\n",
      "|      upperUDF(Name)|                Name|\n",
      "+--------------------+--------------------+\n",
      "|BRAUND, MR. OWEN ...|Braund, Mr. Owen ...|\n",
      "|CUMINGS, MRS. JOH...|Cumings, Mrs. Joh...|\n",
      "|HEIKKINEN, MISS. ...|Heikkinen, Miss. ...|\n",
      "|FUTRELLE, MRS. JA...|Futrelle, Mrs. Ja...|\n",
      "|ALLEN, MR. WILLIA...|Allen, Mr. Willia...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDFs ajudam plugar funções complexas no SQL.\n",
    "\n",
    "def uppercase(str):\n",
    "    return str.upper()\n",
    "\n",
    "def ml_model(data):\n",
    "    return model(data)\n",
    "\n",
    "\n",
    "spark.udf.register(\"machinelearning_model\", uppercase)\n",
    "\n",
    "spark.sql(\"SELECT machine_learning_model(Name), Name from table\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/spark/spark-write-dataframe-to-csv-file/\n",
    "\n",
    "spark.sql(\"SELECT upperUDF(Name), Age from table\").write.option(\"header\", True).format(\"csv\").save(\"names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/spark/spark-read-and-write-json-file/\n",
    "\n",
    "\n",
    "spark.sql(\"SELECT age, count(*) as age_count FROM table WHERE survived == 1 GROUP BY age ORDER BY age_count DESC\").write.format(\"json\").save(\"ages.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\n",
    "    \n",
    "\n",
    "# Saving data to a JDBC source\n",
    "#jdbcDF.write \\\n",
    "#    .format(\"jdbc\") \\\n",
    "#    .option(\"url\", \"jdbc:postgresql:dbserver\") \\\n",
    "#    .option(\"dbtable\", \"schema.tablename\") \\\n",
    "#    .option(\"user\", \"username\") \\\n",
    "#    .option(\"password\", \"password\") \\\n",
    "#    .save()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
