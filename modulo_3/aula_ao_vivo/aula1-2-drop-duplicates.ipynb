{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3c89ccd9-3488-4251-9eac-f61997825486",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+-------------------+\n",
      "| id| nome|        cidade|               data|\n",
      "+---+-----+--------------+-------------------+\n",
      "|  1|pedro|    uberlandia|2021-01-01 00:00:00|\n",
      "|  1|pedro|       uberaba|2022-01-01 00:00:00|\n",
      "|  1|pedro|belo horizonte|2023-01-01 00:00:00|\n",
      "|  1|pedro|belo horizonte|2023-01-01 00:00:00|\n",
      "|  2|maria|       niteroi|2021-01-01 00:00:00|\n",
      "|  2|maria|rio de janeiro|2023-01-01 00:00:00|\n",
      "|  2|maria|   nova iguacu|2021-01-01 00:00:00|\n",
      "|  3| josé|     são paulo|2023-01-01 00:00:00|\n",
      "|  3| josé|     são paulo|2021-01-01 00:00:00|\n",
      "|  3| josé|     são paulo|2022-01-01 00:00:00|\n",
      "+---+-----+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=xw4a9qbOh-Q\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "   .builder  \\\n",
    "   .appName(\"Aula Interativa 1 - Engenharia de Dados - Apache Spark\") \\\n",
    "   .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"arquivos/aula1-2-duplicates.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Em projetos de migração de dados, duplicatas ao longo do processo são comuns\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a620bcbb-dcd8-45d5-a2fe-b2bbfd0fffbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+-------------------+\n",
      "| id| nome|        cidade|               data|\n",
      "+---+-----+--------------+-------------------+\n",
      "|  1|pedro|       uberaba|2022-01-01 00:00:00|\n",
      "|  2|maria|       niteroi|2021-01-01 00:00:00|\n",
      "|  1|pedro|belo horizonte|2023-01-01 00:00:00|\n",
      "|  2|maria|   nova iguacu|2021-01-01 00:00:00|\n",
      "|  3| josé|     são paulo|2022-01-01 00:00:00|\n",
      "|  3| josé|     são paulo|2021-01-01 00:00:00|\n",
      "|  1|pedro|    uberlandia|2021-01-01 00:00:00|\n",
      "|  2|maria|rio de janeiro|2023-01-01 00:00:00|\n",
      "|  3| josé|     são paulo|2023-01-01 00:00:00|\n",
      "+---+-----+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Não resolve!\n",
    "df.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0135d973-5758-4b53-b7f1-daca91d3b785",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+-------------------+\n",
      "| id| nome|        cidade|               data|\n",
      "+---+-----+--------------+-------------------+\n",
      "|  1|pedro|belo horizonte|2023-01-01 00:00:00|\n",
      "|  2|maria|rio de janeiro|2023-01-01 00:00:00|\n",
      "|  3| josé|     são paulo|2023-01-01 00:00:00|\n",
      "+---+-----+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVA 1:\n",
    "# drop_duplicates\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Primeira data é mantida!\n",
    "df.orderBy(col(\"data\").desc()).dropDuplicates([\"id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+-------------------+---------+\n",
      "| id| nome|        cidade|               data|rowNumber|\n",
      "+---+-----+--------------+-------------------+---------+\n",
      "|  1|pedro|belo horizonte|2023-01-01 00:00:00|        1|\n",
      "|  1|pedro|belo horizonte|2023-01-01 00:00:00|        2|\n",
      "|  1|pedro|       uberaba|2022-01-01 00:00:00|        3|\n",
      "|  1|pedro|    uberlandia|2021-01-01 00:00:00|        4|\n",
      "|  2|maria|rio de janeiro|2023-01-01 00:00:00|        1|\n",
      "|  2|maria|       niteroi|2021-01-01 00:00:00|        2|\n",
      "|  2|maria|   nova iguacu|2021-01-01 00:00:00|        3|\n",
      "|  3| josé|     são paulo|2023-01-01 00:00:00|        1|\n",
      "|  3| josé|     são paulo|2022-01-01 00:00:00|        2|\n",
      "|  3| josé|     são paulo|2021-01-01 00:00:00|        3|\n",
      "+---+-----+--------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVA 2:\n",
    "# window functions\n",
    "# Window functions operate on a group of rows\n",
    "# row_number() is a window function\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "df = df.withColumn(\"rowNumber\",row_number().over(Window.partitionBy(\"id\").orderBy(col(\"data\").desc())))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+-------------------+---------+\n",
      "| id| nome|        cidade|               data|rowNumber|\n",
      "+---+-----+--------------+-------------------+---------+\n",
      "|  1|pedro|belo horizonte|2023-01-01 00:00:00|        1|\n",
      "|  2|maria|rio de janeiro|2023-01-01 00:00:00|        1|\n",
      "|  3| josé|     são paulo|2023-01-01 00:00:00|        1|\n",
      "+---+-----+--------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"rownumber=1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+-------------------+---------+\n",
      "| id| nome|        cidade|               data|rowNumber|\n",
      "+---+-----+--------------+-------------------+---------+\n",
      "|  1|pedro|belo horizonte|2023-01-01 00:00:00|        2|\n",
      "|  1|pedro|       uberaba|2022-01-01 00:00:00|        3|\n",
      "|  1|pedro|    uberlandia|2021-01-01 00:00:00|        4|\n",
      "|  2|maria|       niteroi|2021-01-01 00:00:00|        2|\n",
      "|  2|maria|   nova iguacu|2021-01-01 00:00:00|        3|\n",
      "|  3| josé|     são paulo|2022-01-01 00:00:00|        2|\n",
      "|  3| josé|     são paulo|2021-01-01 00:00:00|        3|\n",
      "+---+-----+--------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"rownumber > 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "skip first few rows while creating pyspark dataframe",
   "notebookOrigID": 570886976656679,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
